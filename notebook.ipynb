{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/homebrew/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/homebrew/lib/python3.11/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, utils\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from skimage import io, transform\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "scale = 28\n",
    "distortion_scale = 0.3\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomRotation(degrees=[-45, 45]),\n",
    "    transforms.RandomPerspective(distortion_scale=0.3, p=0.5),\n",
    "    transforms.Normalize(mean=0.1307, std=0.3081)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = MNIST(root='./data', download=True, train=True, transform=transform)\n",
    "train_subset_indices = list(range(0, len(train_dataset), 2))\n",
    "train_subset = Subset(train_dataset, train_subset_indices)\n",
    "\n",
    "test_dataset = MNIST(root='./data', download=True, train=False, transform=transform)\n",
    "test_subset_indices = list(range(0, len(test_dataset), 2))\n",
    "test_subset = Subset(test_dataset, test_subset_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 64\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(train_subset, batch_size=batch, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_subset, batch_size=batch, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def feature_map_dim(input_dim, padding, kernel_size, stride):\n",
    "\n",
    "  output_dim = math.floor((input_dim + 2 * padding - 1 * (kernel_size - 1) - 1) / stride) + 1\n",
    "  return output_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1. Initialize 2 convolution blocks (consists of a convolution layer, an activation function, a MaxPooling layer)\n",
    "\n",
    "        # Convolution Block 1\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=5, kernel_size=3, stride=1)\n",
    "        self.mp1 = nn.MaxPool2d(kernel_size=2) # default stride is the same as the kernel size\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Convolution Block 2 \n",
    "        self.conv2 = nn.Conv2d(in_channels=5, out_channels=10, kernel_size=3, stride=1)\n",
    "        self.mp2 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=250, out_features=100)\n",
    "        self.fc2 = nn.Linear(in_features=100, out_features=50)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        outputs = self.conv1(x)\n",
    "        outputs = self.mp1(outputs)\n",
    "        outputs = self.relu(outputs)\n",
    "        outputs = self.conv2(outputs)\n",
    "        outputs = self.mp2(outputs)\n",
    "        outputs = self.relu(outputs)\n",
    "\n",
    "        outputs = self.flatten(outputs)\n",
    "\n",
    "        outputs = self.relu(self.fc1(outputs))\n",
    "        outputs = self.relu(self.fc2(outputs))\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn.parameters(), lr=1e-4) \n",
    "epoch = 10\n",
    "\n",
    "acc_LIST_CNN = []\n",
    "loss_LIST_CNN = []\n",
    "\n",
    "for epoch in range(epoch):\n",
    "  running_loss = 0.0\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  for inputs, labels in train_loader:\n",
    "      labels = labels.type(torch.LongTensor)\n",
    "      inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      outputs = cnn(inputs)\n",
    "\n",
    "      predictions = torch.argmax(outputs, dim=1)\n",
    "\n",
    "      loss = criterion(outputs, labels)\n",
    "\n",
    "      loss.backward()\n",
    "\n",
    "      optimizer.step()\n",
    "\n",
    "      running_loss += loss.item()\n",
    "\n",
    "      correct += (predictions == labels).sum().detach().cpu().numpy()\n",
    "      total += len(labels)\n",
    "\n",
    "  accuracy = (correct/total) * 100\n",
    "  acc_LIST_CNN.append(accuracy)\n",
    "\n",
    "  loss_LIST_CNN.append(running_loss / len(train_loader))\n",
    "\n",
    "  print(\"The loss for Epoch {} is: {}, Accuracy = {}\".format(epoch, running_loss/len(train_loader), accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        labels = labels.type(torch.LongTensor) \n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = cnn(images)\n",
    "\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        correct += (predictions == labels).sum().detach().cpu().numpy()\n",
    "        total += len(labels)\n",
    "\n",
    "test_acc_CNN = 100 * correct / total\n",
    "\n",
    "print(f'Test Accuracy: ' + str(test_acc_CNN.item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
